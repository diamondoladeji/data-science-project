{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Common Libraries",
   "id": "8bdc8d930d46d413"
  },
  {
   "cell_type": "code",
   "id": "ddc140bc",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json, os, gzip\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import AgglomerativeClustering, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants",
   "id": "cd56a66493cabaed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "FIGURE_SAVE_PATH: str = \"./figures\"\n",
    "TABLE_SAVE_PATH: str = \"./table_results\"\n",
    "DATASET_PATH: str = \"./datasets\"\n",
    "FILES:list[str] = [\"dblp-ref-0.json\", \"dblp-ref-1.json\", \"dblp-ref-2.json\", \"dblp-ref-3.json\"]\n",
    "\n",
    "# Sampling length our dataset is too big this makes it more manageable to work with\n",
    "# 2000 per year is a good representation of our dataset\n",
    "DEV_MODE:bool = False\n",
    "SAMPLES_PER_YEAR:int = 250 if DEV_MODE else 2000"
   ],
   "id": "8f617eb071ed5b08",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73621ad9",
   "metadata": {},
   "source": "### Load the Dataset"
  },
  {
   "cell_type": "code",
   "id": "6ab8612d",
   "metadata": {},
   "source": [
    "# loading data\n",
    "def read_json_lines(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def load_all(files=FILES):\n",
    "    rows = []\n",
    "    for fp in files:\n",
    "        for rec in read_json_lines(f\"./datasets/{fp}\"):\n",
    "            rows.append(rec)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = load_all()\n",
    "\n",
    "as_type = {\n",
    "    \"year\": \"category\",\n",
    "    \"n_citation\": np.uint32,\n",
    "}\n",
    "\n",
    "df = df.astype(as_type)\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(10)",
   "id": "95e7daf8adab7c9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b5cef81e",
   "metadata": {},
   "source": "## Task 1: Preprocessing & Feature Generation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Preprocess & Sample The Dataset",
   "id": "193ca86f8d1380d7"
  },
  {
   "cell_type": "code",
   "id": "49d02f9d",
   "metadata": {},
   "source": [
    "# Drop Missing years\n",
    "df = df[(df[\"title\"].notna()) & (df[\"year\"].notna())]\n",
    "\n",
    "\n",
    "# Filter out rare venues\n",
    "venue_counts = df[\"venue\"].value_counts()\n",
    "df = df[df[\"venue\"].isin(venue_counts[venue_counts >= 50].index)]\n",
    "\n",
    "# Filter out missing venues\n",
    "print(len(df[df['venue'].str.strip() == \"\"]))\n",
    "df = df[df['venue'].str.strip() != \"\"]\n",
    "\n",
    "# as we saw later when doing simple EDA the papers with n_citation = 50 are likely a data scraping error as it shift the distribution outside a power like distribution\n",
    "# SEE:\n",
    "# https://link.springer.com/article/10.1007/s100510050359\n",
    "df_clean_source = df[df['n_citation'] != 50]\n",
    "\n",
    "\n",
    "print(f\"Sampling {SAMPLES_PER_YEAR} papers per year...\")\n",
    "\n",
    "sampled_df = (\n",
    "    df_clean_source.groupby('year', observed = True)\n",
    "    .apply(lambda x : x.sample(min(len(x), SAMPLES_PER_YEAR)), include_groups = False)\n",
    "    .reset_index(level = 'year')\n",
    ")\n",
    "\n",
    "# Handle normalizing or fixing the dataset on the sample since it will be less\n",
    "# computationally expensive\n",
    "\n",
    "# Replace missing abstracts with empty string\n",
    "sampled_df[\"abstract\"] = sampled_df[\"abstract\"].fillna(\"\")\n",
    "\n",
    "# Normalize Text\n",
    "sampled_df[\"title\"] = sampled_df[\"title\"].str.lower()\n",
    "sampled_df[\"abstract\"] = sampled_df[\"abstract\"].str.lower()\n",
    "\n",
    "# Combine title and abstract as one feature\n",
    "sampled_df['text'] = sampled_df['title'] + \" \" + sampled_df['abstract']\n",
    "\n",
    "# Reset Index so df rows align with matrix rows (0 to N)\n",
    "sampled_df = sampled_df.reset_index(drop = True)\n",
    "\n",
    "print(f\"Original Shape: {df.shape}\")\n",
    "print(f\"Sampled Shape: {sampled_df.shape}\")\n",
    "sampled_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Generate Black Box Features\n",
    "These are for our clustering and classification tasks in our proposal."
   ],
   "id": "f6392ae84560e3ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### TF-IDF Pipeline",
   "id": "a61644e3259bce54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# In task 2 I Found more stop words so we add this here in order to get better topic clustering\n",
    "\n",
    "my_stop_words = list(TfidfVectorizer(stop_words=\"english\").get_stop_words())\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=my_stop_words,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "X_tfidf = tfidf.fit_transform(sampled_df[\"text\"])"
   ],
   "id": "3fe05ad6174a632c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Important: You only really need to run this once after that its meaningless\n",
    "\n",
    "We test here to find the optimal n_dimension by trying to maximize the silhoutte score for sparse text datasets its usually advised for the n_dimension to be around 50-350\n"
   ],
   "id": "13f9bd6690275ed5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "components_to_test = [50, 75, 100, 125, 150, 175, 200, 225, 250, 275, 300]\n",
    "n_clusters = 10\n",
    "max_sil_samples = 5000   # subsample for silhouette to keep it fast\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_comp in components_to_test:\n",
    "    print(f\"\\n=== Testing {n_comp} components ===\")\n",
    "\n",
    "    svd = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "    X_red = svd.fit_transform(X_tfidf)\n",
    "\n",
    "    cum_var = svd.explained_variance_ratio_.sum()\n",
    "    print(f\"Cumulative explained variance: {cum_var:.4f}\")\n",
    "\n",
    "    # K-means on reduced data\n",
    "    km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    labels = km.fit_predict(X_red)\n",
    "\n",
    "    # Subsample for silhouette (it’s O(n^2))\n",
    "    if X_red.shape[0] > max_sil_samples:\n",
    "        idx = np.random.choice(X_red.shape[0], max_sil_samples, replace=False)\n",
    "        sil = silhouette_score(X_red[idx], labels[idx])\n",
    "    else:\n",
    "        sil = silhouette_score(X_red, labels)\n",
    "\n",
    "    print(f\"Silhouette score: {sil:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"n_components\": n_comp,\n",
    "        \"cum_explained_var\": cum_var,\n",
    "        \"silhouette\": sil\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "# plot the tradeoff\n",
    "fig, ax1 = plt.subplots(figsize=(9,5))\n",
    "\n",
    "ax1.plot(results_df[\"n_components\"], results_df[\"silhouette\"],\n",
    "         marker=\"o\", label=\"Silhouette score\")\n",
    "ax1.set_xlabel(\"Number of components\")\n",
    "ax1.set_ylabel(\"Silhouette score\")\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results_df[\"n_components\"], results_df[\"cum_explained_var\"],\n",
    "         marker=\"s\", color=\"tab:orange\", label=\"Cumulative variance\")\n",
    "ax2.set_ylabel(\"Cumulative explained variance\")\n",
    "\n",
    "# Combine legends\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines + lines2, labels + labels2, loc=\"best\")\n",
    "\n",
    "plt.title(\"Tradeoff: PCA components vs clustering & variance\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/pca_ncomponent_tradeoff.png\")\n",
    "plt.show()\n"
   ],
   "id": "7b2f9c911bb9012a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### PCA Preprocess Pipeline",
   "id": "f1a29267d72a70df"
  },
  {
   "cell_type": "code",
   "id": "fad2c047",
   "metadata": {},
   "source": [
    "# PCA Feature Pipeline\n",
    "\n",
    "# Check if PCA columns already exist and drop them so we don't get duplicates\n",
    "cols_to_drop = [c for c in sampled_df.columns if c.startswith('pca_') or c == 'cluster']\n",
    "sampled_df = sampled_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Run PCA\n",
    "OPTIMAL_N_COMPONENT = 50\n",
    "# See above plot to find the optimal n_components\n",
    "pca = PCA(n_components=OPTIMAL_N_COMPONENT)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Create a df from PCA results and attach it to sampled_df\n",
    "pca_cols = [f\"pca_{i}\" for i in range(50)]\n",
    "pca_df = pd.DataFrame(X_pca, columns=pca_cols)\n",
    "sampled_df = pd.concat([sampled_df, pca_df], axis=1)\n",
    "\n",
    "# Author and Venue Embeddings\n",
    "# Venue embeddings\n",
    "venue_emb = sampled_df.groupby(\"venue\")[pca_cols].mean()\n",
    "\n",
    "# Author embeddings\n",
    "authors_exploded = []\n",
    "authors_emb = []\n",
    "authors_exploded = sampled_df.explode('authors')\n",
    "author_emb = authors_exploded.groupby('authors')[pca_cols].mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Generate Interpretable Features\n",
    "These are the metadata features for trend analysis and network analysis"
   ],
   "id": "2bae38bdf969a356"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Number of Authors (Collaboration Size)\n",
    "sampled_df['num_authors'] = sampled_df['authors'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Title Length (Character Count)\n",
    "sampled_df['title_length'] = sampled_df['title'].apply(lambda x: len(str(x)))\n",
    "\n",
    "# Number of References (How many papers this paper cites)\n",
    "sampled_df['num_refs'] = sampled_df['references'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "\n",
    "# Citation Velocity (Citations per year)\n",
    "sampled_df['citation_velocity'] = sampled_df['n_citation'] / (2018 - sampled_df['year'].astype(int) + 1)\n",
    "print(sampled_df['citation_velocity'].max())\n",
    "\n",
    "# Change the types as well to reduce memory\n",
    "as_type = {\n",
    "    \"num_authors\": \"category\",\n",
    "    \"title_length\": np.uint16,\n",
    "    \"num_refs\": np.uint8,\n",
    "    \"citation_velocity\": np.float32,\n",
    "}\n",
    "\n",
    "sampled_df[[\"title\", 'year', 'num_authors', 'num_refs', 'citation_velocity']].head()"
   ],
   "id": "52e94d603f6516af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### EDA Validation for our cleaned sampled dataset\n",
    "\n",
    "To see if our preprocessing pipeline generated quality data"
   ],
   "id": "6aac66e40fbe67b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The \"Power Law\" Check (Citation Distribution)\n",
    "# See https://link.springer.com/article/10.1007/s100510050359\n",
    "# This is now done above in the preprocessing check\n",
    "# Preprocessed above to remove n_citation = 50 outlier\n",
    "# ARTIFACT_VAL = 50\n",
    "# clean_df = sampled_df[sampled_df['n_citation'] != ARTIFACT_VAL]\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(sampled_df['n_citation'] + 1, log_scale=True, kde=True, color='teal')\n",
    "plt.title('Distribution of Citations (Log Scale)')\n",
    "plt.xlabel('Number of Citations (Log)')\n",
    "plt.ylabel('Count of Papers')\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/eda_citation_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Group by year and get mean authors\n",
    "auth_trend = sampled_df.groupby('year')['num_authors'].mean().reset_index()\n",
    "sns.lineplot(data=auth_trend, x='year', y='num_authors', marker='o')\n",
    "plt.title('Average Number of Authors per Paper (Collaboration Trend)')\n",
    "plt.ylabel('Avg Authors')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/eda_collaboration_trend.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Get top 20 venues\n",
    "top_venues = sampled_df['venue'].value_counts().nlargest(20).index\n",
    "# Filter data to only those venues for plotting\n",
    "sns.countplot(y='venue', data=sampled_df[sampled_df['venue'].isin(top_venues)], order=top_venues, palette='viridis')\n",
    "plt.title('Top 20 Most Frequent Venues')\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/eda_top_venues.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Select only numeric metadata\n",
    "meta_cols = ['year', 'n_citation', 'num_authors', 'title_length', 'num_refs', 'citation_velocity']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sampled_df[meta_cols].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Metadata Features')\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/eda_corr_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "print(sampled_df[meta_cols].corr())"
   ],
   "id": "6e50172627adc95",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a919d75b",
   "metadata": {},
   "source": "## Task 2: Topic Clustering"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### K-Means Pipeline",
   "id": "cd2f60d7b7730bbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run clustering and name the topics",
   "id": "f3504bb5dcffabe5"
  },
  {
   "cell_type": "code",
   "id": "b970f7f9",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "OPTIMAL_K = 8 # We chose this k for interpretability\n",
    "\n",
    "# Fit Final Model\n",
    "print(f\"Clustering into {OPTIMAL_K} topics...\")\n",
    "# Use X_pca for the clustering itself (efficient)\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_K, random_state=42, n_init=10)\n",
    "sampled_df['cluster'] = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Interpret Clusters (Keyword Extraction)\n",
    "print(\"Extracting top keywords for each topic...\")\n",
    "\n",
    "# We need the original TF-IDF vocabulary to convert numbers back to words\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "# Group papers by cluster\n",
    "for i in range(OPTIMAL_K):\n",
    "    # Get the indices of papers in this cluster\n",
    "    cluster_indices = sampled_df[sampled_df['cluster'] == i].index\n",
    "\n",
    "    # Calculate the AVERAGE TF-IDF vector for this cluster\n",
    "    # (We use the X_tfidf matrix from Task 1, slicing by the cluster's rows)\n",
    "    cluster_center = X_tfidf[cluster_indices].mean(axis=0)\n",
    "\n",
    "    # Convert matrix to 1D array\n",
    "    cluster_center_arr = np.asarray(cluster_center).flatten()\n",
    "\n",
    "    # Get indices of the top 10 highest weighted words\n",
    "    top_indices = cluster_center_arr.argsort()[::-1][:10]\n",
    "\n",
    "    # Map indices to words\n",
    "    top_keywords = [feature_names[ind] for ind in top_indices]\n",
    "\n",
    "    print(f\"\\n--- Cluster {i} ({len(cluster_indices)} papers) ---\")\n",
    "    print(f\"Keywords: {', '.join(top_keywords)}\")\n",
    "    # Show a sample title to verify\n",
    "    print(f\"Sample Title: {sampled_df[sampled_df['cluster'] == i]['title'].iloc[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For report:\n",
    "\n",
    "Unsupervised clustering revealed a distinct group of non-english papers (cluster 1). We utilized this insight to filter the dataset, removing 186 foreign language documents to improve downstream topic modeling\n",
    "\n",
    "\n",
    "Important: ONlY RUN THIS ONCE TO REMOVE THE GERMAN PAPERS!\n",
    "\n",
    "Now we remove cluster x and rerun the pipeline\n",
    "\n",
    "Re run TF-IDF -> PCA -> K-means pipeline"
   ],
   "id": "3ba189ff00f27fe3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify the german cluster index\n",
    "sampled_df[sampled_df['cluster'] == 6]"
   ],
   "id": "9593317723d6def4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "german_cluster_id = 6\n",
    "print(f\"Removing {len(sampled_df[sampled_df['cluster'] == german_cluster_id])} non-English papers...\")\n",
    "\n",
    "sampled_df = sampled_df[sampled_df['cluster'] != german_cluster_id].copy()\n",
    "\n",
    "sampled_df = sampled_df.reset_index(drop=True)"
   ],
   "id": "db8415469a73d196",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualize the clusters with new human-readable labels you might need to change the cluster_names list see above for the output result",
   "id": "4924d8901584177f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update cluster names based on the output we get above\n",
    "cluster_names = {\n",
    "    0: 'Numerical Computing',      # matrix, inversion, polynomial\n",
    "    1: 'General Modeling & Logic', # (The large catch-all cluster)\n",
    "    2: 'Robotics & Control',       # robot, controller, manipulator\n",
    "    3: 'Optimization & Theory',    # linear programming, equations\n",
    "    4: 'Software & Info Systems',  # language, software, user, design\n",
    "    5: 'Graph Theory',             # vertices, edge, connected\n",
    "    6: 'Networking & Wireless',    # wireless, routing, protocol\n",
    "    7: 'Editorial/Historical'      # letters to editor, algol, acm\n",
    "}\n",
    "\n",
    "# Apply mapping\n",
    "sampled_df['label'] = sampled_df['cluster'].map(cluster_names)\n",
    "\n",
    "# 3. Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='pca_0',\n",
    "    y='pca_1',\n",
    "    hue='label',\n",
    "    data=sampled_df,\n",
    "    palette='tab10',\n",
    "    alpha=0.9,\n",
    "    s=60             # Marker size\n",
    ")\n",
    "plt.title('DBLP Research Landscape: Topic Clusters', fontsize=15)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Research Topic')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/topic_clustering_pca.png\")\n",
    "plt.show()"
   ],
   "id": "892e691581c61822",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "print(\"Running t-SNE to visualize clusters (this takes a moment)...\")\n",
    "\n",
    "# We use the same X_pca data, but project it differently\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, random_state=42)\n",
    "tsne_results = tsne.fit_transform(X_pca)\n",
    "\n",
    "# Add t-SNE results to the dataframe\n",
    "sampled_df['tsne_x'] = tsne_results[:, 0]\n",
    "sampled_df['tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "# Plotting the t-SNE result\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x='tsne_x',\n",
    "    y='tsne_y',\n",
    "    hue='label',\n",
    "    data=sampled_df,\n",
    "    palette='tab10', # Stronger colors\n",
    "    alpha=0.7,\n",
    "    s=40,\n",
    "    legend='full'\n",
    ")\n",
    "\n",
    "plt.title('t-SNE Visualization of Research Topics', fontsize=16)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{FIGURE_SAVE_PATH}/topic_clustering_t-SNE.png\")\n",
    "plt.show()"
   ],
   "id": "4d96efe804564006",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f610ba7",
   "metadata": {},
   "source": [
    "### 5. Temporal Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "0f81f925",
   "metadata": {},
   "source": [
    "# drop missing values\n",
    "df = df.dropna(subset=['title', 'year', 'authors', 'references'])\n",
    "\n",
    "# fill missing \n",
    "df['venue'] = df['venue'].fillna('Unknown Venue')\n",
    "df['abstract'] = df['abstract'].fillna(\"\")\n",
    "\n",
    "# make sure year is integer\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df = df[df['year'].between(1950, 2017)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caae8d66",
   "metadata": {},
   "source": [
    "# create numerical features for clustering\n",
    "df[\"text\"] = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).str.lower()\n",
    "\n",
    "# vectorize text using hashing\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=50000, stop_words=\"english\", alternate_sign=False)\n",
    "X_hv = hv.fit_transform(df[\"text\"])\n",
    "\n",
    "# dimensionality reduction using TruncatedSVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=150, random_state=42)\n",
    "X_svd = svd.fit_transform(X_hv)\n",
    "X_svd = Normalizer(copy=False).fit_transform(X_svd)\n",
    "\n",
    "# clustering using KMeans\n",
    "K = 50\n",
    "kmeans = MiniBatchKMeans(n_clusters=K, random_state=42, batch_size=1024)\n",
    "labels = kmeans.fit_predict(X_svd)\n",
    "df['cluster'] = labels"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2aa328bc",
   "metadata": {},
   "source": [
    "# temporal trend analysis \n",
    "trend = df.groupby(['year', 'cluster']).size().rename('count').reset_index()\n",
    "\n",
    "year_totals = trend.groupby('year')['count'].sum().rename('year_total')\n",
    "trend = trend.merge(year_totals, on='year')\n",
    "trend[\"share\"] = trend[\"count\"] / trend[\"year_total\"]\n",
    "\n",
    "# visualization of trends\n",
    "sns.set(style=\"whitegrid\", context=\"talk\", palette=\"muted\")\n",
    "top_clusters = df[\"cluster\"].value_counts().head(10).index\n",
    "trend_top = trend[trend[\"cluster\"].isin(top_clusters)]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "for cluster in top_clusters:\n",
    "    sub = trend_top[trend_top[\"cluster\"] == cluster].sort_values(\"year\")\n",
    "    plt.plot(sub[\"year\"], sub[\"share\"], label=f\"Cluster {cluster}\", linewidth=2)\n",
    "\n",
    "plt.axvline(2010, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "plt.title(\"Temporal Trends of Top 10 Clusters in Publications\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Share of Publications\")\n",
    "plt.legend(loc=\"upper left\", ncol=2, fontsize=10, frameon=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0a1ca73e",
   "metadata": {},
   "source": [
    "# annotate fastest rising clusters post-2010\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def cluster_slope(cluster_id):\n",
    "    sub = trend[trend[\"cluster\"] == cluster_id]\n",
    "    X = sub[\"year\"].values.reshape(-1,1)\n",
    "    y = sub[\"share\"].values\n",
    "    if len(sub) < 5:\n",
    "        return np.nan\n",
    "    return LinearRegression().fit(X,y).coef_[0]\n",
    "\n",
    "slopes = {c: cluster_slope(c) for c in top_clusters}\n",
    "fast_risers = sorted(slopes, key=slopes.get, reverse=True)[:3]\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "for c in top_clusters:\n",
    "    sub = trend_top[trend_top[\"cluster\"] == c].sort_values(\"year\")\n",
    "    plt.plot(sub[\"year\"], sub[\"share\"], label=f\"Cluster {c}\", linewidth=2)\n",
    "\n",
    "plt.axvline(2010, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for c in fast_risers:\n",
    "    sub = trend_top[trend_top[\"cluster\"] == c].sort_values(\"year\")\n",
    "    if not sub.empty:\n",
    "        x, y = sub[\"year\"].iloc[-1], sub[\"share\"].iloc[-1]\n",
    "        plt.annotate(\n",
    "            f\"↑ Cluster {c}\",\n",
    "            xy=(x,y),\n",
    "            xytext=(x+1,y+0.01),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"gray\")\n",
    "        )\n",
    "\n",
    "plt.title(\"Topic trends with post-2010 marker and fast risers\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Share of papers\")\n",
    "plt.legend(loc=\"upper left\", ncol=2, fontsize=10, frameon=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf7f474b",
   "metadata": {},
   "source": "### Task 4: Predictive Modeling (Classification)"
  },
  {
   "cell_type": "code",
   "id": "4af7fddc",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "85120f88",
   "metadata": {},
   "source": [
    "### 7. Network Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
